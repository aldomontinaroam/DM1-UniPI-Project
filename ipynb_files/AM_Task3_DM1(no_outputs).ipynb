{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MesubbmxZwFf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hVM7o7hSyo8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "plt.style.use(\"fast\")\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.cm as cm\n",
        "from scipy import interp\n",
        "\n",
        "!pip install scikit-plot -q\n",
        "import scikitplot\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.__version__)"
      ],
      "metadata": {
        "id": "u6gJK8OelYuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XdBE7V6XZazD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset encoded but not scaled."
      ],
      "metadata": {
        "id": "cZtMbArBhwU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = '/content/drive/MyDrive/UNIPI/DM1/DATASET: Spotify/DM1 - Project/files/dm1_df_understanding_NOTSCALED.csv'\n",
        "data = pd.read_csv(datasetPath)\n",
        "\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "Zkm3JU7YZcyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to easily map object variables to the integers with which they were replaced (we are talking about `name`, `artists`, `album_name`, and `genre`), we decided to save to disk a dictionary of dictionaries, containing each integer associated with the original string. This way we can see what `genre` each integer in the genre column is associated with."
      ],
      "metadata": {
        "id": "BcIkp_JK1klN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Carica il dizionario inverso da disco\n",
        "with open('/content/drive/MyDrive/UNIPI/DM1/DATASET: Spotify/DM1 - Project/files/inverse_column_dicts.pkl', 'rb') as f:\n",
        "    loaded_inverse_column_dicts = pickle.load(f)\n",
        "\n",
        "genre_dict = loaded_inverse_column_dicts['genre']\n",
        "genre_dict"
      ],
      "metadata": {
        "id": "ia2YB6-A0ovq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy(deep=True)"
      ],
      "metadata": {
        "id": "-1Glcni-Zrj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gnNqj5KpfoUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['name','artists', 'album_name'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XyQ5H3AMjBOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding 'key'\n",
        "df = pd.get_dummies(df, columns=['key'])"
      ],
      "metadata": {
        "id": "oNSSjfpnIneJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Xbhkk0rjizAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "9YPoj32h-52f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.countplot(data=df, x=df['genre'])"
      ],
      "metadata": {
        "id": "WLIhJAe8vy9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST SET"
      ],
      "metadata": {
        "id": "RiOwQfHsHLlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testSetPath = '/content/drive/MyDrive/UNIPI/DM1/DATASET: Spotify/DM1 - Project/files/test.csv'\n",
        "test_df = pd.read_csv(testSetPath)\n",
        "\n",
        "test_df.head(3)"
      ],
      "metadata": {
        "id": "diBIUTC5HM2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing\n",
        "test_df.drop(['name','artists', 'album_name','time_signature','popularity_confidence','features_duration_ms','n_beats'], axis=1, inplace=True)\n",
        "test_df['duration_min'] = test_df['duration_ms'] / 60000\n",
        "test_df.drop('duration_ms', axis=1, inplace=True)\n",
        "test_df['explicit'] = [int(x) for x in test_df['explicit']] # bool->int (binary)\n",
        "test_df['key'] = [int(x) for x in test_df['key']] # float -> int\n",
        "\n",
        "data_modeNoNull = test_df.dropna(subset=['mode'], axis=0)\n",
        "mode_1_records = test_df[test_df['mode'] == 1].shape[0]\n",
        "mode_0_records = test_df[test_df['mode'] == 0].shape[0]\n",
        "percent_1 = mode_1_records / data_modeNoNull.shape[0]\n",
        "percent_0 = mode_0_records / data_modeNoNull.shape[0]\n",
        "null_count_mode = test_df['mode'].isnull().sum()\n",
        "count_1 = int(null_count_mode * (percent_1 / 100))\n",
        "count_0 = null_count_mode - count_1\n",
        "random_values = np.random.choice([1, 0], size=null_count_mode, p=[percent_1, percent_0])\n",
        "test_df.loc[test_df['mode'].isnull(), 'mode'] = random_values\n",
        "test_df['mode'] = [int(x) for x in test_df['mode']] # float->int (binary)\n",
        "\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "3JzN1SN6Jm8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni i generi nel training set\n",
        "train_genres = set(genre_dict.values())\n",
        "\n",
        "# Ottieni i generi unici nel test set\n",
        "test_genres = set(test_df['genre'].unique())\n",
        "\n",
        "# Confronta i generi\n",
        "common_genres = train_genres.intersection(test_genres)\n",
        "train_only_genres = train_genres.difference(test_genres)\n",
        "test_only_genres = test_genres.difference(train_genres)\n",
        "\n",
        "print(\"Generi comuni:\", common_genres)\n",
        "print(\"Generi solo nel training set:\", train_only_genres)\n",
        "print(\"Generi solo nel test set:\", test_only_genres)"
      ],
      "metadata": {
        "id": "Qnc-geUbM5fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverti il dizionario\n",
        "genre_dict_test = {v: k for k, v in genre_dict.items()}\n",
        "# Ora mappa i generi del test set agli interi corrispondenti\n",
        "test_df['genre'] = test_df['genre'].map(genre_dict_test)"
      ],
      "metadata": {
        "id": "IK_28CJHLING"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.get_dummies(test_df, columns=['key'])\n",
        "print(test_df.shape, df.shape)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "TAan_MT1QHIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TARGET: `genre`"
      ],
      "metadata": {
        "id": "wt5o1LvQZ4M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'genre' column (target) from the DataFrame\n",
        "df_without_target = df.drop('genre', axis=1)\n",
        "\n",
        "# Convert the DataFrame without the target column to a NumPy array\n",
        "X = df_without_target.values\n",
        "\n",
        "# Save column names\n",
        "columns = df_without_target.columns.tolist()\n",
        "\n",
        "# Convert target column to NumPy array\n",
        "y = np.array(df['genre'])\n",
        "\n",
        "# Check unique values and their counts\n",
        "np.unique(y, return_counts=True)"
      ],
      "metadata": {
        "id": "nZB2hjZrfKSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stratify parameter is used to ensure that the division of training and test data maintains the same class distribution as in the original data set."
      ],
      "metadata": {
        "id": "ZvyWHK-GfEKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARTITIONING\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "EvHimyMseS6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "norm = StandardScaler()\n",
        "norm.fit(X_train)\n",
        "\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)"
      ],
      "metadata": {
        "id": "aZOsR9Lrdwyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "iiQ16tzgaDCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model with default parameters\n",
        "base_knn = KNeighborsClassifier()\n",
        "base_knn.fit(X_train_norm, y_train)\n",
        "\n",
        "base_y_test_pred = base_knn.predict(X_test_norm)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, base_y_test_pred):.2f}\")"
      ],
      "metadata": {
        "id": "Wo6UfeR2uWNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "average=\"micro\": This calculates the metrics globally by counting the total true positives, false negatives, and false positives1. It aggregates the contributions of all classes to compute the average metric2. In other words, it computes the F1 score across all classes, which is useful if you have class imbalance2. This means that micro F1 score gives equal importance to each observation"
      ],
      "metadata": {
        "id": "xsO8g9_6wYdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"F1: {f1_score(y_test, base_y_test_pred, average='micro'):.2f}\")"
      ],
      "metadata": {
        "id": "U39i62otuh64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "PNkJp_k0cOG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses cross-validation to evaluate the performance of the model. Instead of dividing the data into a training set and a test set, cross-validation divides the data into `k` groups or \"folds.\" The model is then trained on `k-1` fold and tested on the remaining fold. This process is repeated `k` times, with each fold used once as a test set. The mean and standard deviation of accuracy over all folds are then calculated and plotted in a graph for each value of `n_neighbors`.\n",
        "\n",
        "We set `k=5`.\n",
        "\n",
        "The vertical bars in the graph generated by the second code represent the standard deviation of model accuracy for each value of n_neighbors during cross-validation. These bars are called \"error bars.\"\n",
        "\n",
        "\n",
        "The standard deviation is a measure of the variability or dispersion of the data. In this case, it indicates how much the accuracy of the model varies between different folds of the cross-validation. If the error bar for a certain value of n_neighbors is large, it means that the accuracy of the model varies greatly among different folds. If the error bar is small, it means that the accuracy of the model is about the same for all folds.\n",
        "\n",
        "\n",
        "Error bars help to understand how reliable the average accuracy estimate is. If the error bars are small, we can be more confident that the average accuracy accurately represents the performance of the model. If the error bars are large, the average accuracy may not be a good estimate of model performance."
      ],
      "metadata": {
        "id": "lPTeU_1ed7Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 2/3 min\n",
        "\n",
        "n_neighbors = range(1,100)\n",
        "avg_scores = list()\n",
        "std_scores = list()\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "for n in n_neighbors:\n",
        "    clf = KNeighborsClassifier(n_neighbors=n)\n",
        "    scores = cross_val_score(clf, X_train_norm, y_train, cv=5)\n",
        "    avg_scores.append(np.mean(scores))\n",
        "    std_scores.append(np.std(scores))\n",
        "\n",
        "max_score_index = np.argmax(avg_scores)\n",
        "max_n_neighbors = n_neighbors[max_score_index]\n",
        "max_score = avg_scores[max_score_index]\n",
        "\n",
        "#plt.plot(avg_scores)\n",
        "plt.errorbar(range(len(n_neighbors)), y=avg_scores, yerr=std_scores, marker='o', color='green')\n",
        "plt.xticks(range(len(n_neighbors)), n_neighbors)\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "\n",
        "plt.text(0.1, 0.9, f'Max accuracy at n_neighbors={max_n_neighbors}',\n",
        "         transform=plt.gca().transAxes, bbox=dict(facecolor='teal', alpha=0.5))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jCqGhxsKckQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=max_n_neighbors)\n",
        "knn.fit(X_train_norm, y_train)\n",
        "y_test_pred = knn.predict(X_test_norm)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.2f}\")"
      ],
      "metadata": {
        "id": "WdJsfaAfdjrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above 10/12 neighbors the accuracy stabilizes around a value of about 0.43 (for the chosen range of n_neighbors between 1 and 50 the maximum accuracy, 0.44, is reached for n_neighbors=14). Selecting a range of neighbors between 10 and 15 we can evaluate othe hyperparameters using a gridSearch."
      ],
      "metadata": {
        "id": "NvUXQE5VhnZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
      ],
      "metadata": {
        "id": "twRxs2QPhvdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# ~14min\n",
        "\n",
        "param_grid = {\n",
        "    \"n_neighbors\": np.arange(10, 20),\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "    \"metric\": [\"euclidean\", \"cityblock\"],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=RepeatedStratifiedKFold(random_state=0),\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid.fit(X_train_norm, y_train)\n",
        "best_knn = grid.best_estimator_\n",
        "\n",
        "print(grid.best_params_, grid.best_score_)\n",
        "\n",
        "best_y_test_pred = best_knn.predict(X_test_norm)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, best_y_test_pred):.2f}\")\n",
        "print(f\"F1: {f1_score(y_test, best_y_test_pred, average='micro'):.2f}\")"
      ],
      "metadata": {
        "id": "A8-N7KAJhxmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The configuration `{'metric': 'cityblock', 'n_neighbors': 14, 'weights': 'distance'}` performs best, with a final accuracy of about 0.48."
      ],
      "metadata": {
        "id": "JzONzJRasHUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''plt.figure(figsize=(20,20))\n",
        "cf = confusion_matrix(y_test, best_y_test_pred)\n",
        "sns.heatmap(cf, annot=True, cmap=\"Greens\")\n",
        "plt.xlabel(\"True\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "c5eNmNwzw22s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(grid.cv_results_)\n",
        "results[\"metric_weight\"] = results[\"param_metric\"] + \", \" + results[\"param_weights\"]\n",
        "sns.lineplot(\n",
        "    data=results, x=\"param_n_neighbors\", y=\"mean_test_score\", hue=\"metric_weight\"\n",
        ")"
      ],
      "metadata": {
        "id": "Gzav7KwJifNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For multiple classes we can choose one as positive and all the others as negative and compute precision, recall, F-score. Repeating the process for all classes we can obtain an average accuracy."
      ],
      "metadata": {
        "id": "NvcGHINE4ziW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance evaluation"
      ],
      "metadata": {
        "id": "3OmkvJcP0WN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Binarize the output\n",
        "\n",
        "'''\n",
        "LabelBinarizer is used to transform categorical labels into a binary format,\n",
        "often referred to as one-hot encoding. It transforms categorical labels into a binary\n",
        "matrix. Each label is replaced with a row containing 0s and a single 1 at the corresponding\n",
        "class column, indicating the presence of that class.\n",
        "'''\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "best_y_test_pred_bin = lb.transform(best_y_test_pred)\n",
        "\n",
        "# Compute precision, recall, F-score for each class\n",
        "report = classification_report(y_test_bin, best_y_test_pred_bin, output_dict=True)\n",
        "\n",
        "# Compute average accuracy\n",
        "avg_accuracy = sum([report[str(i)]['f1-score'] for i in range(len(lb.classes_))]) / len(lb.classes_)\n",
        "\n",
        "print(f\"Average Accuracy: {avg_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "npNc5V0t4xap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(y_test, y_score, n_classes):\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "             label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                   ''.format(roc_auc[\"micro\"]),\n",
        "             color='green', linestyle=':', linewidth=8)\n",
        "\n",
        "    colors = cm.Greys(np.linspace(0, 1, n_classes))\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic to Multi-Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CGlggdrawSn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.cm as cm\n",
        "from scipy import interp\n",
        "\n",
        "n_classes = len(df['genre'].unique())\n",
        "best_y_test_pred_proba = best_knn.predict_proba(X_test_norm)\n",
        "best_y_test_pred_labels = best_y_test_pred_proba.argmax(axis=1)\n",
        "\n",
        "print(f\"F1:{f1_score(y_test, best_y_test_pred_labels, labels=[1], average='micro'):.2f}\")\n",
        "\n",
        "plot_roc(y_test, best_y_test_pred_proba, n_classes=n_classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kYl-B1cWuFR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def plot_precision_recall(y_test, y_score, n_classes):\n",
        "\n",
        "    # Compute Precision-Recall curve and area for each class\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    average_precision = dict()\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
        "        average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
        "\n",
        "    # Compute micro-average Precision-Recall curve and area\n",
        "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test_bin.ravel(), y_score.ravel())\n",
        "    average_precision[\"micro\"] = average_precision_score(y_test_bin, y_score, average=\"micro\")\n",
        "\n",
        "    # Plot all Precision-Recall curves\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
        "             label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
        "                   ''.format(average_precision[\"micro\"]),\n",
        "             color='green', linestyle=':', linewidth=8)\n",
        "\n",
        "    colors = cm.Greys(np.linspace(0, 1, n_classes))\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
        "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, average_precision[i]))\n",
        "\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall curve to Multi-Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall(y_test, best_y_test_pred_proba, n_classes=n_classes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3tkBbbt8yetF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Repeated Holdout"
      ],
      "metadata": {
        "id": "Fp6ElYRW0aJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each iteration of the loop, we are creating a new instance of the best model (`rh_clf = grid.best_estimator_`) before training it on the training data. This should ensure that there is no information from the test set that is used when training the model."
      ],
      "metadata": {
        "id": "gzmon9oo4PoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 3min\n",
        "\n",
        "N = 100\n",
        "err = 0\n",
        "\n",
        "for i in range(N):\n",
        "    X_rh_train, X_rh_test, y_rh_train, y_rh_test = train_test_split(X, y, test_size=0.4, stratify=y)\n",
        "\n",
        "    # normalize train set\n",
        "    norm.fit(X_rh_train)\n",
        "    X_rh_train_norm = norm.transform(X_rh_train)\n",
        "    X_rh_test_norm = norm.transform(X_rh_test)\n",
        "\n",
        "    rh_clf = grid.best_estimator_\n",
        "    rh_clf.fit(X_rh_train_norm, y_rh_train)\n",
        "\n",
        "    #Â computing error\n",
        "    acc = rh_clf.score(X_rh_test_norm, y_rh_test)\n",
        "    err += 1 - acc\n",
        "\n",
        "print(f\"Overall error estimate: {err/N:.2f}\")"
      ],
      "metadata": {
        "id": "x1rlWZlwy4x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"HO Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "fYuGknz06PvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total error we got is about 0.52. This means that, on average, the model made errors 52% of the time during 100 repeated holdout iterations. In other words, the model misclassified 52% of the instances in the test set."
      ],
      "metadata": {
        "id": "QE0Y7f7g0I7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-Validation"
      ],
      "metadata": {
        "id": "PXOML2H50cJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "k = 10\n",
        "\n",
        "scores = cross_val_score(best_knn, X_train_norm, y_train, cv=k)\n",
        "print(f\"Overall error estimate: {1 - scores.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "id": "NdyVDj_a0fUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST"
      ],
      "metadata": {
        "id": "RajhZqeCGr11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'genre' column (target) from the DataFrame\n",
        "df_without_target_EXT = test_df.drop('genre', axis=1)\n",
        "\n",
        "# Convert the DataFrame without the target column to a NumPy array\n",
        "X_testEXT = df_without_target_EXT.values\n",
        "\n",
        "# Convert target column to NumPy array\n",
        "y_ext = np.array(test_df['genre'])\n",
        "\n",
        "X_testEXT_norm = norm.transform(X_testEXT)\n",
        "\n",
        "scores_EXT = cross_val_score(best_knn, X_testEXT_norm, y_ext, cv=10)\n",
        "print('RESULTS ON EXTERNAL TEST SET:')\n",
        "print(f\"Overall error estimate: {1 - scores_EXT.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_EXT.mean(), scores_EXT.std()))"
      ],
      "metadata": {
        "id": "AOvHU48GGt_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "OnBsQQdBaEa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "ejaRQdO2CKPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAUSSIAN_NB\n",
        "'''\n",
        "Utilizzo solo variabili continue\n",
        "'''\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "nb_cont_features = [\n",
        "    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'n_bars', 'duration_min'\n",
        "]\n",
        "\n",
        "X_nbg = df[nb_cont_features].values\n",
        "\n",
        "X_train_gauss, X_test_gauss, y_train_gauss, y_test_gauss = train_test_split(\n",
        "    X_nbg, y, test_size=0.3, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "nbg = GaussianNB()\n",
        "nbg.fit(X_train_gauss, y_train_gauss)\n",
        "\n",
        "y_pred_nbg = nbg.predict(X_test_gauss)\n",
        "\n",
        "print(classification_report(y_test_gauss, y_pred_nbg))\n",
        "print(roc_auc_score(y_test_gauss, nbg.predict_proba(X_test_gauss), multi_class=\"ovr\", average=\"micro\"))\n",
        "scores_nbg = cross_val_score(nbg, X_train_gauss, y_train_gauss, cv=k)\n",
        "print(f\"Overall error estimate: {1 - scores_nbg.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_nbg.mean(), scores_nbg.std()))"
      ],
      "metadata": {
        "id": "PtrpZdM1CVWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CATEGORICAL_NB\n",
        "\n",
        "non_cat_columns = [\n",
        "    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'n_bars', 'duration_min'\n",
        "]\n",
        "\n",
        "X_noncat = df[non_cat_columns].values\n",
        "\n",
        "X_train_noncat, X_test_noncat, y_train_noncat, y_test_noncat = train_test_split(\n",
        "    X_noncat, y, test_size=0.3, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "X_train_cat = list()\n",
        "for column_idx in range(X_train_noncat.shape[1]):\n",
        "    X_train_cat.append(pd.qcut(X_train_noncat[:, column_idx], q=4, labels=False, duplicates='drop'))\n",
        "X_train_cat = np.array(X_train_cat).T\n",
        "\n",
        "X_test_cat = list()\n",
        "for column_idx in range(X_test_noncat.shape[1]):\n",
        "    X_test_cat.append(pd.qcut(X_test_noncat[:, column_idx], q=4, labels=False, duplicates='drop'))\n",
        "X_test_cat = np.array(X_test_cat).T\n",
        "\n",
        "print(X_train_cat.shape, X_test_cat.shape)\n",
        "\n",
        "nbc = CategoricalNB()\n",
        "nbc.fit(X_train_cat, y_train_noncat)\n",
        "\n",
        "y_pred_nbc = nbc.predict(X_test_cat)\n",
        "\n",
        "print(classification_report(y_test_noncat, y_pred_nbc))\n",
        "print('ROC/AUC score: ',roc_auc_score(y_test_noncat, nbc.predict_proba(X_test_cat), multi_class=\"ovr\", average=\"micro\"))\n",
        "scores_nbc = cross_val_score(nbc, X_train_cat, y_train_noncat, cv=10)\n",
        "print(f\"Overall error estimate: {1 - scores_nbc.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_nbc.mean(), scores_nbc.std()))"
      ],
      "metadata": {
        "id": "Q-OJSOZapvS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees"
      ],
      "metadata": {
        "id": "1RlX1b5DaGXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        ")"
      ],
      "metadata": {
        "id": "Uk5cD_muN3Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 5min\n",
        "\n",
        "# BASE MODEL\n",
        "base_dt = DecisionTreeClassifier()\n",
        "\n",
        "base_dt.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(20, 4), dpi=300)\n",
        "plot_tree(base_dt, feature_names=columns, filled=True)\n",
        "plt.show()\n",
        "\n",
        "y_train_pred = base_dt.predict(X_train)\n",
        "y_test_pred = base_dt.predict(X_test)\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(cf, annot=True, cmap=\"Greens\")\n",
        "plt.xlabel(\"True\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "zipped = zip(columns, base_dt.feature_importances_)\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for col, imp in zipped:\n",
        "    print(col, imp)\n",
        "\n",
        "print(roc_auc_score(y_test, base_dt.predict_proba(X_test), multi_class=\"ovr\", average=\"micro\"))\n",
        "scores_baseDT = cross_val_score(base_dt, X_train, y_train, cv=10)\n",
        "print(f\"Overall error estimate: {1 - scores_baseDT.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_baseDT.mean(), scores_baseDT.std()))"
      ],
      "metadata": {
        "id": "qjOUZZxCOmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "s0stRw5iSc-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "max_depth int, default=None\n",
        "\n",
        "The maximum depth of the tree. If None, then nodes are expanded\n",
        "until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "'''\n",
        "\n",
        "def depth_param_graph(interval, train_X, train_y, cv):\n",
        "  max_depths = interval\n",
        "  avg_scores = list()\n",
        "  std_scores = list()\n",
        "  scores = list()\n",
        "\n",
        "  for max_depth in max_depths:\n",
        "      dt = DecisionTreeClassifier(max_depth=max_depth)\n",
        "      cross_val_scores = cross_val_score(dt, train_X, train_y, cv=cv)\n",
        "      avg_scores.append(np.mean(cross_val_scores))\n",
        "      std_scores.append(np.std(cross_val_scores))\n",
        "      avg_score = np.mean(cross_val_scores)\n",
        "      std_score = np.std(cross_val_scores)\n",
        "      scores.append((max_depth, avg_score, std_score))\n",
        "\n",
        "  scores.sort(key=lambda x: x[1], reverse=True)\n",
        "  range_depth = sorted([t[0] for t in scores[:5]])\n",
        "  print(range_depth)\n",
        "\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.errorbar(range(len(max_depths)), y=avg_scores, yerr=std_scores, marker='o')\n",
        "  plt.xticks(range(len(max_depths)), max_depths)\n",
        "  plt.xlabel(\"max_depths\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.show()\n",
        "\n",
        "  return range_depth\n",
        "\n",
        "range_depth = depth_param_graph(interval=range(1, 51, 1), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "zTxMrGvpQshz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "min_samples_split int or float, default=2\n",
        "\n",
        "The minimum number of samples required to split an internal node:\n",
        "\n",
        "- If int, then consider min_samples_split as the minimum number.\n",
        "- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples)\n",
        "  are the minimum number of samples for each split.\n",
        "'''\n",
        "\n",
        "def split_param_graph(interval, train_X, train_y, cv):\n",
        "  min_samples_splits = interval\n",
        "  avg_scores = list()\n",
        "  std_scores = list()\n",
        "  scores = list()\n",
        "\n",
        "  for min_samples_split in min_samples_splits:\n",
        "     dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
        "     cross_val_scores = cross_val_score(dt, train_X, train_y, cv=cv)\n",
        "     avg_scores.append(np.mean(cross_val_scores))\n",
        "     std_scores.append(np.std(cross_val_scores))\n",
        "     avg_score = np.mean(cross_val_scores)\n",
        "     std_score = np.std(cross_val_scores)\n",
        "     scores.append((min_samples_split, avg_score, std_score))\n",
        "\n",
        "  scores.sort(key=lambda x: x[1], reverse=True)\n",
        "  range_split = sorted([t[0] for t in scores[:5]])\n",
        "  print(range_split)\n",
        "\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.errorbar(range(len(min_samples_splits)), y=avg_scores, yerr=std_scores, marker='o')\n",
        "  plt.xticks(range(len(min_samples_splits)), min_samples_splits)\n",
        "  plt.xlabel(\"min_samples_split\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.show()\n",
        "\n",
        "  return range_split\n",
        "\n",
        "range_split = split_param_graph(interval=range(2, 102, 2), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "SWxdwVZOUS9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "min_samples_leaf int or float, default=1\n",
        "\n",
        "The minimum number of samples required to be at a leaf node. A split point at any depth\n",
        "will only be considered if it leaves at least min_samples_leaf training samples in each of\n",
        "the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "\n",
        "- If int, then consider min_samples_leaf as the minimum number.\n",
        "- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
        "'''\n",
        "\n",
        "def leaf_param_graph(interval, train_X, train_y, cv):\n",
        "  min_samples_leafs = interval\n",
        "  avg_scores = list()\n",
        "  std_scores = list()\n",
        "  scores = list()\n",
        "\n",
        "  for min_samples_leaf in min_samples_leafs:\n",
        "     dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
        "     cross_val_scores = cross_val_score(dt, train_X, train_y, cv=cv)\n",
        "     avg_scores.append(np.mean(cross_val_scores))\n",
        "     std_scores.append(np.std(cross_val_scores))\n",
        "     avg_score = np.mean(cross_val_scores)\n",
        "     std_score = np.std(cross_val_scores)\n",
        "     scores.append((min_samples_leaf, avg_score, std_score))\n",
        "\n",
        "  scores.sort(key=lambda x: x[1], reverse=True)\n",
        "  range_leaf = sorted([t[0] for t in scores[:5]])\n",
        "  print(range_leaf)\n",
        "\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.errorbar(range(len(min_samples_leafs)), y=avg_scores, yerr=std_scores, marker='o')\n",
        "  plt.xticks(range(len(min_samples_leafs)), min_samples_leafs)\n",
        "  plt.xlabel(\"min_samples_leaf\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.show()\n",
        "\n",
        "  return range_leaf\n",
        "\n",
        "range_leaf = leaf_param_graph(interval=range(1, 101, 1), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "4mmioHjiXOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GridSearch"
      ],
      "metadata": {
        "id": "7p8MyP2Obtpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ],
      "metadata": {
        "id": "j1RSGb-FbtBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 35min\n",
        "\n",
        "# TODO: implement tqdm\n",
        "\n",
        "param_list = { # based on graphs above\n",
        "    'max_depth': range_depth,\n",
        "    'min_samples_split': range_split,\n",
        "    'min_samples_leaf': range_leaf,\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter':['best','random']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    DecisionTreeClassifier(),\n",
        "    param_distributions=param_list,\n",
        "    cv=RepeatedStratifiedKFold(random_state=0),\n",
        "    n_jobs=-1, # number of jobs to run in parallel, -1 means using all processors\n",
        "    refit=True, # allows using predict directly with this estimator\n",
        "    n_iter=500 # the total size of your parameter space is the product of the number of values for each parameter; n_iter <= size (now 500)\n",
        "    #verbose=2\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "dt = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "n811fZFebxo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_params_, random_search.best_score_)\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "fzUkbhyuhizG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "fig, axs = plt.subplots(2)\n",
        "\n",
        "sns.lineplot(data=results, x=\"param_max_depth\", y=\"mean_test_score\", ax=axs[0])\n",
        "axs[0].set_title('max_depth')\n",
        "\n",
        "sns.lineplot(data=results, x=\"param_min_samples_leaf\", y=\"mean_test_score\", ax=axs[1])\n",
        "axs[1].set_title('min_samples_leaf')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HOclBqqXiI9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = zip(columns, dt.feature_importances_)\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for col, imp in zipped:\n",
        "    print(col, imp)"
      ],
      "metadata": {
        "id": "dCRaETGDicma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 4), dpi=300)\n",
        "plot_tree(dt, feature_names=columns, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bERB-NB1i761"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)"
      ],
      "metadata": {
        "id": "fZ34Fxtc7h1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.font_manager as fm\n",
        "fm.findSystemFonts(fontpaths=None, fontext='ttf')"
      ],
      "metadata": {
        "id": "euxjr4ow9Pq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 6min\n",
        "\n",
        "import dtreeviz\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set the default font to one that is available on your system\n",
        "plt.rcParams['font.family'] = 'LiberationMono-Regular'  # Example font\n",
        "\n",
        "# Now you can create your visualization with dtreeviz\n",
        "viz = dtreeviz.model(dt, X_train, y_train,\n",
        "                     feature_names=columns,\n",
        "                     class_names=list(np.unique(y)))\n",
        "\n",
        "viz.view()"
      ],
      "metadata": {
        "id": "lZBDPyp09N_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz.view().save(\"decision_tree_genre.svg\")"
      ],
      "metadata": {
        "id": "poJn9yLn_-ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.tree_.children_left[6] = -1\n",
        "dt.tree_.children_right[6]  = -1\n",
        "\n",
        "viz2 = dtreeviz.model(dt, X_train, y_train,\n",
        "                     feature_names=columns,\n",
        "                     class_names=list(np.unique(y)))\n",
        "\n",
        "viz2.view().save(\"decision_tree_genre_pruned.svg\")"
      ],
      "metadata": {
        "id": "DKegsyt-B7Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz2.view()"
      ],
      "metadata": {
        "id": "LZutWSKsCsim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "dot_data = tree.export_graphviz(dt, out_file=None,\n",
        "                    feature_names=columns,\n",
        "                    class_names=[str(i) for i in np.unique(y)],\n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "5yp0yd_BF6mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(dt, out_file=None,\n",
        "                    feature_names=columns,\n",
        "                    class_names=[str(i) for i in np.unique(y)],\n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True, max_depth=2, rotate=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "e5K5I8VgSIUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.render(\"decision_tree_genre_graphviz\", format=\"png\")"
      ],
      "metadata": {
        "id": "owbZdiEGWBy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q treeinterpreter\n",
        "from treeinterpreter import treeinterpreter as ti\n",
        "\n",
        "# Use TreeInterpreter to interpret the model's predictions\n",
        "prediction, bias, contributions = ti.predict(dt, X_test)\n",
        "\n",
        "# Print the results for the first instance in the test set\n",
        "print(\"Prediction:\", prediction[0])\n",
        "print(\"Bias (trainset prior):\", bias[0])\n",
        "print(\"Feature contributions:\")\n",
        "for c, feature in zip(contributions[0], columns):\n",
        "    print(feature, c)"
      ],
      "metadata": {
        "id": "bQMlGfw3SfEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TreeInterpreter results provided can be interpreted as follows:\n",
        "\n",
        "- Prediction: This is the predicted probability distribution over the classes for a given instance. In our case, the model predicts that the instance belongs to the third class with a probability of 0.34375, the sixth class with a probability of 0.15625, and so on.\n",
        "\n",
        "- Bias (trainset prior): This is the prior probability distribution over the classes, which is typically the distribution of the classes in the training set. In our case, all classes have a prior probability of 0.05.\n",
        "\n",
        "- Feature contributions: This shows how much each feature contributes to the shift from the prior probability to the predicted probability for each class. The contribution of a feature can be positive (increasing the probability of a class), negative (decreasing the probability), or zero (no effect).\n",
        "\n",
        "For example, the `popularity` feature increases the probability of the third class by 0.08729017 and decreases the probability of the second class by 0.07425332. Similarly, the `danceability` feature increases the probability of the third class by 0.320599022 and decreases the probability of the tenth class by 0.132828714.\n",
        "\n",
        "The sum of the bias and the feature contributions for each class should be equal to the predicted probability for that class."
      ],
      "metadata": {
        "id": "VGcmAvFIT1ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ccp_aplhas"
      ],
      "metadata": {
        "id": "Jupx0sHxgV7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal cost complexity pruning recursively finds the node with the âweakest linkâ. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. <br>\n",
        "Sklearn decision tree offers a function that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves."
      ],
      "metadata": {
        "id": "1b9N4LqEWbGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code computes the cost complexity pruning path associated with the decision tree on the training data. The cost_complexity_pruning_path function returns the effective alphas (the complexity parameter used for Minimal Cost-Complexity Pruning) and the corresponding total leaf impurities at each step of the pruning process. The total impurity of a tree is the sum of the impurity of its leaves.\n",
        "\n",
        "Then, it plots the total impurity of leaves versus effective alpha for the training set. This helps in visualizing how the total impurity decreases with increasing alpha."
      ],
      "metadata": {
        "id": "GvyWLEGnjc0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax.set_xlabel(\"effective alpha\")\n",
        "ax.set_ylabel(\"total impurity of leaves\")\n",
        "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
      ],
      "metadata": {
        "id": "tJPXLcW6gZJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each effective alpha, a Decision Tree Classifier is trained with that alpha and the best parameters found by random_search. All the classifiers are stored in the clfs list."
      ],
      "metadata": {
        "id": "KW15GsYrji1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, **random_search.best_params_)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "print(\n",
        "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
        "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "kaKzYczKggSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code plots the number of nodes and the depth of the tree as a function of alpha. This helps in understanding how the complexity of the tree decreases with increasing alpha."
      ],
      "metadata": {
        "id": "HJJePnuejm3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "node_counts = [clf.tree_.node_count for clf in clfs]\n",
        "depth = [clf.tree_.max_depth for clf in clfs]\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[0].set_xlabel(\"alpha\")\n",
        "ax[0].set_ylabel(\"number of nodes\")\n",
        "ax[0].set_title(\"Number of nodes vs alpha\")\n",
        "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[1].set_xlabel(\"alpha\")\n",
        "ax[1].set_ylabel(\"depth of tree\")\n",
        "ax[1].set_title(\"Depth vs alpha\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "Jh_UV47zgiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each classifier in clfs, the training and testing accuracies are computed and plotted against alpha. This helps in understanding how the modelâs performance varies with its complexity."
      ],
      "metadata": {
        "id": "XIMWY3krjpq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S4OAWw1qgkKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final Decision Tree Classifier is trained with an alpha of (???) and the best parameters found by random_search.\n",
        "\n",
        "The trained modelâs performance is evaluated on the test set using accuracy and F1-score. A classification report is also printed which provides detailed performance metrics."
      ],
      "metadata": {
        "id": "vnTMWkDujrs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha, imp in zip(ccp_alphas, impurities):\n",
        "    print(alpha, imp)"
      ],
      "metadata": {
        "id": "mcC9ON4GXasw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index of the best test score\n",
        "idx_best_alpha = np.argmax(test_scores)\n",
        "\n",
        "# Get the best alpha value\n",
        "best_alpha = ccp_alphas[idx_best_alpha]\n",
        "\n",
        "print(\"Best ccp_alpha value is: \", best_alpha)"
      ],
      "metadata": {
        "id": "179Z5T5JZIVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtp = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha, **random_search.best_params_) #choose ccp based on graphs (?)\n",
        "dtp.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred = dtp.predict(X_test)\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "print()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print()\n",
        "\n",
        "print('================== Classification Report ==================')\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "bmcPnlk3gmT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the ROC curve and Precision-Recall curve for the final model are plotted. These plots provide a comprehensive view of the modelâs performance across different thresholds."
      ],
      "metadata": {
        "id": "PFrbexzIj2Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scikitplot.metrics import plot_roc\n",
        "from scikitplot.metrics import plot_precision_recall\n",
        "\n",
        "plot_roc(y_test, dtp.predict_proba(X_test))\n",
        "plt.show()\n",
        "\n",
        "plot_precision_recall(y_test, dtp.predict_proba(X_test))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPVr6Qmsg_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Micro-average ROC curve area (AUC-ROC) = 0.88: The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The Area Under the ROC Curve (AUC-ROC) is a single-value summary of the ROC curve that ranges from 0 to 1. An AUC-ROC of 0.88 indicates a good performance of the model in distinguishing between the classes across all thresholds. A perfect classifier would have an AUC-ROC of 1.\n",
        "\n",
        "- Micro-average Precision-Recall curve area (AUC-PR) = 0.463: The PR curve plots precision (PPV) against recall (TPR) at various threshold settings. The Area Under the PR Curve (AUC-PR) is a single-value summary of the PR curve that ranges from 0 to 1. An AUC-PR of 0.463 indicates a moderate performance of the model in terms of precision and recall. A perfect classifier would have an AUC-PR of 1.\n",
        "\n",
        "Itâs important to note that the AUC-PR can be more informative than the AUC-ROC when dealing with imbalanced datasets. However, you mentioned that your dataset is balanced, so both metrics are equally informative in this case.\n",
        "\n",
        "In conclusion, your model shows good performance in terms of the AUC-ROC, but there is room for improvement in terms of the AUC-PR. This could indicate that while the model is good at ranking the classes correctly (as measured by the AUC-ROC), it might struggle with achieving high precision and recall simultaneously (as measured by the AUC-PR)."
      ],
      "metadata": {
        "id": "d1qyvS-Jb1hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, this code is a comprehensive demonstration of training a Decision Tree Classifier with Cost Complexity Pruning, and evaluating its performance using various metrics. It also provides visual insights into how the modelâs complexity and performance vary with the pruning parameter (alpha)."
      ],
      "metadata": {
        "id": "7l647fsOj3kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modelâs performance varies significantly across different classes. Some classes (like Class 8 and Class 13) have high precision, recall, and F1-scores, indicating that the model performs well on these classes. However, other classes (like Class 7 and Class 19) have low scores, suggesting that the model struggles with these classes."
      ],
      "metadata": {
        "id": "KsVR6vQ_bFwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection\n",
        "\n",
        "Filter Methods: These methods are generally used as a preprocessing step. They use statistical measures to score each feature and select the top-scoring features. Examples include:\n",
        "\n",
        "- Correlation Coefficient: Features with high correlation with the target variable are selected.\n",
        "- Information Gain: This method is used to measure the reduction in entropy (randomness or impurity) in the target variable due to the information contained in the features."
      ],
      "metadata": {
        "id": "xZOgUkKKZ9Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fs = df.copy(deep=True)"
      ],
      "metadata": {
        "id": "knwUVMApsmkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming that df is your DataFrame and 'genre' is your target variable\n",
        "corr_coef = df_fs.corr()\n",
        "\n",
        "# Correlation with output variable\n",
        "cor_target = abs(corr_coef[\"genre\"])\n",
        "\n",
        "# Sort the features by correlation\n",
        "sorted_cor_target = cor_target.sort_values(ascending=False)\n",
        "\n",
        "# Print the sorted features\n",
        "print(sorted_cor_target)"
      ],
      "metadata": {
        "id": "t73A6daIlYlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mutual information (MI) between two variables is a measure of the reduction in uncertainty (or entropy) about one variable given the knowledge of another. In the context of feature selection, MI measures the dependency between each feature and the target variable. A higher MI between a feature and the target variable means that the feature provides more information that can help in predicting the target variable, thus reducing its entropy."
      ],
      "metadata": {
        "id": "vpGXM0HUnjHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "X_fs = df_fs.drop('genre', axis=1)\n",
        "y_fs = df_fs['genre']\n",
        "\n",
        "# Assuming that X is your feature matrix and y is your target variable\n",
        "mi = mutual_info_classif(X_fs, y_fs)\n",
        "\n",
        "# Create a DataFrame with the scores\n",
        "mi_scores = pd.DataFrame({'Feature': X_fs.columns, 'Score': mi})\n",
        "\n",
        "mi_sorted = mi_scores.sort_values(by='Score', ascending=False)\n",
        "print(mi_sorted)"
      ],
      "metadata": {
        "id": "tDn2NGoymyD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the two methods\n",
        "combined_scores = pd.DataFrame({\n",
        "    'Feature': X_fs.columns,\n",
        "    'Correlation': cor_target[X_fs.columns],\n",
        "    'Mutual Information': mi_scores.set_index('Feature')['Score']\n",
        "})\n",
        "\n",
        "# Handle any missing values\n",
        "combined_scores = combined_scores.dropna()\n",
        "\n",
        "# You can then sort this DataFrame by the sum of the two scores\n",
        "combined_scores['Combined Score'] = (combined_scores['Correlation'] + combined_scores['Mutual Information'])/2\n",
        "combined_scores = combined_scores.sort_values(by='Combined Score', ascending=False)\n",
        "\n",
        "combined_scores"
      ],
      "metadata": {
        "id": "RM5YbvYosQvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results provided are a list of features with their correlation scores, mutual information, and a combined score. These scores are used to evaluate the importance of features in the model.\n",
        "\n",
        "Here is an interpretation of some of the results:\n",
        "\n",
        "- Danceability: It has a correlation of 0.30 and a mutual information score of 0.34 with the target variable. This suggests that danceability might be an important feature in the model, since it has a relatively high combined score of 0.32.\n",
        "- Popularity: It has a lower correlation of 0.15, but a very high mutual information score of 0.48, which suggests that popularity might capture some complex information that is not captured by correlation alone.\n",
        "- Acousticness, Energy, Valence, Instrumentalness, Loudness: These features have lower correlation and mutual information scores than danceability and popularity, which might suggest that they are less important. However, they could still be useful depending on the specific model and the problem we are trying to solve.\n",
        "- Key_1, Key_10, ..., Key_3: These are categorical features (musical key of a song). They have both correlation and low mutual information, which suggests that they may not be very informative for the model."
      ],
      "metadata": {
        "id": "BLo8tc_WT-oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = combined_scores.nlargest(5, 'Combined Score')['Feature']\n",
        "\n",
        "X_fs_selected = X_fs[top_features]\n",
        "\n",
        "# PARTITIONING\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_fs, y_fs, test_size=0.4, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "norm = StandardScaler()\n",
        "norm.fit(X_train)\n",
        "\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)\n",
        "\n",
        "dt = DecisionTreeClassifier(**random_search.best_params_, ccp_alpha=best_alpha)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "bayv0j7HUXb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "k = 10\n",
        "\n",
        "scores = cross_val_score(dt, X_train_norm, y_train, cv=k)\n",
        "print(f\"Overall error estimate: {1 - scores.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "id": "vupCTibmbsgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(df['genre'].unique())\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "best_y_test_pred_bin = lb.transform(y_test_pred)\n",
        "\n",
        "y_test_pred_proba = dt.predict_proba(X_test_norm)\n",
        "y_test_pred_labels = y_test_pred_proba.argmax(axis=1)\n",
        "\n",
        "print(f\"F1:{f1_score(y_test, y_test_pred_labels, labels=[1], average='micro'):.2f}\")\n",
        "\n",
        "plot_roc(y_test, y_test_pred_proba)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Txe6wxuucWeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "\n",
        "scores_EXT = cross_val_score(dt, X_testEXT_norm, y_ext, cv=10)\n",
        "print('RESULTS ON EXTERNAL TEST SET:')\n",
        "print(f\"Overall error estimate: {1 - scores_EXT.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_EXT.mean(), scores_EXT.std()))"
      ],
      "metadata": {
        "id": "Si-ChXgYetPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TARGET: `popularity`"
      ],
      "metadata": {
        "id": "UdouVKUrR4cG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfpop = df.copy(deep=True) # copy of the original dataset"
      ],
      "metadata": {
        "id": "DsEf0I84XMHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "popularity_bins = [0, 50, 80, 100]\n",
        "popularity_labels = [0, 1, 2] # low, medium, high\n",
        "dfpop['popularity_bin'] = pd.cut(dfpop['popularity'],\n",
        "                                 bins=popularity_bins,\n",
        "                                 labels=popularity_labels,\n",
        "                                 include_lowest=True)\n",
        "dfpop['popularity_bin'] = dfpop['popularity_bin'].astype('int')"
      ],
      "metadata": {
        "id": "IbEODeenW8vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfpop.info()"
      ],
      "metadata": {
        "id": "cPaRYdGWZBON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'genre' column (target) from the DataFrame\n",
        "df_without_target = dfpop.drop(['popularity_bin','popularity'], axis=1)\n",
        "\n",
        "# Convert the DataFrame without the target column to a NumPy array\n",
        "X = df_without_target.values\n",
        "\n",
        "# Save column names\n",
        "columns = df_without_target.columns.tolist()\n",
        "\n",
        "# Convert target column to NumPy array\n",
        "y = np.array(dfpop['popularity_bin'])\n",
        "\n",
        "# Check unique values and their counts\n",
        "np.unique(y, return_counts=True)"
      ],
      "metadata": {
        "id": "wVV2T81GYzy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there are 13022 songs with `low` popularity, 1956 with `medium` and 22 with `high`, according to the choosen intervals."
      ],
      "metadata": {
        "id": "jxWuulqpa4Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARTITIONING and STANDARDIZATION\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "norm = StandardScaler()\n",
        "norm.fit(X_train)\n",
        "\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)"
      ],
      "metadata": {
        "id": "BniLrT6ObMpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees"
      ],
      "metadata": {
        "id": "OIM0pc_EeVDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 2min\n",
        "\n",
        "# BASE MODEL\n",
        "base_dt = DecisionTreeClassifier()\n",
        "\n",
        "base_dt.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(20, 4), dpi=300)\n",
        "plot_tree(base_dt, feature_names=columns, filled=True)\n",
        "plt.show()\n",
        "\n",
        "y_train_pred = base_dt.predict(X_train)\n",
        "y_test_pred = base_dt.predict(X_test)\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(cf, annot=True, cmap=\"Greens\")\n",
        "plt.xlabel(\"True\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "zipped = zip(columns, base_dt.feature_importances_)\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for col, imp in zipped:\n",
        "    print(col, imp)\n",
        "\n",
        "print(roc_auc_score(y_test, base_dt.predict_proba(X_test), multi_class=\"ovr\", average=\"micro\"))\n",
        "scores_baseDT = cross_val_score(base_dt, X_train, y_train, cv=10)\n",
        "print(f\"Overall error estimate: {1 - scores_baseDT.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_baseDT.mean(), scores_baseDT.std()))"
      ],
      "metadata": {
        "id": "pdzASizPeXJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_depth = depth_param_graph(interval=range(1, 51, 1), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "FcwgK23IfNXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_split = split_param_graph(interval=range(2, 102, 2), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "QaoqcHmjhbS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_leaf = leaf_param_graph(interval=range(1, 101, 1), train_X=X_train, train_y=y_train, cv=5)"
      ],
      "metadata": {
        "id": "p9XeXqmmhZxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 8min\n",
        "\n",
        "# Your parameter list remains the same\n",
        "param_list = {\n",
        "    'max_depth': range_depth,\n",
        "    'min_samples_split': range_split,\n",
        "    'min_samples_leaf': range_leaf,\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter':['best','random']\n",
        "}\n",
        "\n",
        "# Create a new instance of RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    DecisionTreeClassifier(),\n",
        "    param_distributions=param_list,\n",
        "    cv=RepeatedStratifiedKFold(random_state=0),\n",
        "    n_jobs=-1, # number of jobs to run in parallel, -1 means using all processors\n",
        "    refit=True, # allows using predict directly with this estimator\n",
        "    n_iter=500 # the total size of your parameter space is the product of the number of values for each parameter; n_iter <= size (now 500)\n",
        "    #verbose=2\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "dt = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "ISuGPyOqi0ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_params_, random_search.best_score_)\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "print()\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print()\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('================== Classification Report ==================')\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print('===========================================================')\n",
        "zipped = zip(columns, dt.feature_importances_)\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for col, imp in zipped:\n",
        "    print(col, imp)"
      ],
      "metadata": {
        "id": "G5QTVnijoywN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scikitplot.metrics import plot_roc\n",
        "from scikitplot.metrics import plot_precision_recall\n",
        "\n",
        "plot_roc(y_test, dt.predict_proba(X_test))\n",
        "plt.show()\n",
        "\n",
        "plot_precision_recall(y_test, dt.predict_proba(X_test))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oYjMpL4Bo28k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Weights"
      ],
      "metadata": {
        "id": "ZmZlfO-rqznc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 4min\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define a range of weights\n",
        "weights = [{0:1, 1:x, 2:y} for x in range(10, 110, 10) for y in range(100, 1100, 100)]\n",
        "\n",
        "# Store the cross-validated scores for each set of weights\n",
        "scores = []\n",
        "\n",
        "# Use tqdm to show progress\n",
        "for weight in tqdm(weights):\n",
        "    dt = DecisionTreeClassifier(class_weight=weight)\n",
        "    score = cross_val_score(dt, X_train, y_train, cv=10, scoring='f1_weighted')\n",
        "    scores.append(score.mean())\n",
        "\n",
        "# Find the weights with the highest score\n",
        "optimal_weights = weights[scores.index(max(scores))]\n",
        "print(optimal_weights)"
      ],
      "metadata": {
        "id": "M1JihJV2q1g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_weighted = DecisionTreeClassifier(random_state=0, **random_search.best_params_, class_weight=optimal_weights)\n",
        "dt_weighted.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dt_weighted.predict(X_train)\n",
        "y_test_pred = dt_weighted.predict(X_test)\n",
        "print()\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print()\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('================== Classification Report ==================')\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print('===========================================================')\n",
        "zipped = zip(columns, dt_weighted.feature_importances_)\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for col, imp in zipped:\n",
        "    print(col, imp)"
      ],
      "metadata": {
        "id": "GGrQ78JBsQ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, dt_weighted.predict_proba(X_test))\n",
        "plt.show()\n",
        "\n",
        "plot_precision_recall(y_test, dt_weighted.predict_proba(X_test))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y6LbXNg-skiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ccp_alphas"
      ],
      "metadata": {
        "id": "ccGGJgyvvy8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = dt_weighted.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, **random_search.best_params_, class_weight=optimal_weights)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "node_counts = [clf.tree_.node_count for clf in clfs]\n",
        "depth = [clf.tree_.max_depth for clf in clfs]\n",
        "\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# Get the index of the best test score\n",
        "idx_best_alpha = np.argmax(test_scores)\n",
        "\n",
        "# Get the best alpha value\n",
        "best_alpha = ccp_alphas[idx_best_alpha]\n",
        "\n",
        "print(\"Best ccp_alpha value is: \", best_alpha)"
      ],
      "metadata": {
        "id": "nZ_KedWuv0db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtp = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha, **random_search.best_params_, class_weight=optimal_weights) #choose ccp based on graphs (?)\n",
        "dtp.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred = dtp.predict(X_test)\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "print()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print()\n",
        "\n",
        "print('================== Classification Report ==================')\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "zStiuzsAwGar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_test, dtp.predict_proba(X_test))\n",
        "plt.show()\n",
        "\n",
        "plot_precision_recall(y_test, dtp.predict_proba(X_test))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4g4zuf0awSfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature selection"
      ],
      "metadata": {
        "id": "P8PEuYxfxDPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# ???\n",
        "\n",
        "df_fs = dfpop.copy(deep=True)\n",
        "df_fs['popularity_bin'] = df_fs['popularity_bin'].astype('int')\n",
        "\n",
        "corr_coef = df_fs.corr()\n",
        "\n",
        "# Correlation with output variable\n",
        "cor_target = abs(corr_coef[\"popularity_bin\"])\n",
        "\n",
        "# Sort the features by correlation\n",
        "sorted_cor_target = cor_target.sort_values(ascending=False)\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "y_fs = df_fs['popularity_bin']\n",
        "X_fs = df_fs.drop(['popularity', 'popularity_bin'], axis=1)\n",
        "\n",
        "# Assuming that X is your feature matrix and y is your target variable\n",
        "mi = mutual_info_classif(X_fs, y_fs)\n",
        "\n",
        "# Create a DataFrame with the scores\n",
        "mi_scores = pd.DataFrame({'Feature': X_fs.columns, 'Score': mi})\n",
        "\n",
        "mi_sorted = mi_scores.sort_values(by='Score', ascending=False)\n",
        "\n",
        "# Combine the two methods\n",
        "combined_scores = pd.DataFrame({\n",
        "    'Feature': X_fs.columns,\n",
        "    'Correlation': cor_target[X_fs.columns],\n",
        "    'Mutual Information': mi_scores.set_index('Feature')['Score']\n",
        "})\n",
        "\n",
        "# Handle any missing values\n",
        "combined_scores = combined_scores.dropna()\n",
        "\n",
        "# You can then sort this DataFrame by the sum of the two scores\n",
        "combined_scores['Combined Score'] = (combined_scores['Correlation'] + combined_scores['Mutual Information'])/2\n",
        "combined_scores = combined_scores.sort_values(by='Combined Score', ascending=False)\n",
        "\n",
        "top_features = combined_scores.nlargest(5, 'Combined Score')['Feature']\n",
        "\n",
        "X_fs_selected = X_fs[top_features]\n",
        "\n",
        "# PARTITIONING\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_fs, y_fs, test_size=0.4, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "norm = StandardScaler()\n",
        "norm.fit(X_train)\n",
        "\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)\n",
        "\n",
        "dt = DecisionTreeClassifier(**random_search.best_params_, ccp_alpha=best_alpha, class_weight=optimal_weights)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "\n",
        "print('Train Accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Train F1-score %s' % f1_score(y_train, y_train_pred, average=None))\n",
        "print()\n",
        "\n",
        "print('Test Accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
        "print('Test F1-score %s' % f1_score(y_test, y_test_pred, average=None))\n",
        "print()\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print()\n",
        "scores = cross_val_score(dt, X_train_norm, y_train, cv=10)\n",
        "print(f\"Overall ERROR estimate: {1 - scores.mean():.2f}\")\n",
        "print('ACCURACY: %0.4f (+/- %0.2f)' % (scores.mean(), scores.std()))\n",
        "\n",
        "\n",
        "n_classes = len(df_fs['popularity_bin'].unique())\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "best_y_test_pred_bin = lb.transform(y_test_pred)\n",
        "\n",
        "y_test_pred_proba = dt.predict_proba(X_test_norm)\n",
        "y_test_pred_labels = y_test_pred_proba.argmax(axis=1)\n",
        "\n",
        "print(f\"F1:{f1_score(y_test, y_test_pred_labels, labels=[1], average='micro'):.2f}\")\n",
        "\n",
        "plot_roc(y_test, y_test_pred_proba)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X11uEgJ-xFKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTERNAL TEST SET\n",
        "test_df['popularity_bin'] = pd.cut(test_df['popularity'],\n",
        "                                 bins=popularity_bins,\n",
        "                                 labels=popularity_labels,\n",
        "                                 include_lowest=True)\n",
        "test_df['popularity_bin'] = test_df['popularity_bin'].astype('int')\n",
        "\n",
        "\n",
        "y_ext = np.array(test_df['popularity_bin'])\n",
        "X_testEXT = test_df.drop(['popularity_bin', 'popularity'], axis=1).values\n",
        "\n",
        "X_testEXT_norm = norm.transform(X_testEXT)\n",
        "\n",
        "scores_EXT = cross_val_score(dt, X_testEXT_norm, y_ext, cv=10)\n",
        "print('RESULTS ON EXTERNAL TEST SET:')\n",
        "print(f\"Overall error estimate: {1 - scores_EXT.mean():.2f}\")\n",
        "print('Accuracy: %0.4f (+/- %0.2f)' % (scores_EXT.mean(), scores_EXT.std()))"
      ],
      "metadata": {
        "id": "Tla8Mp130EJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}